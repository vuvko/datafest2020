Всем привет, меня зовут Андрей Шадриков. Я работаю с Сбербанке и занимаюсь задачами компьютерного зрения.
Сегодня я хотел бы поделиться некоторыми историями, связанными с переводом нейронных сетей из одного фреймворка в другой, какие проблемы при этом возникали, как их можно решить, и что вообще лучше делать, чтобы было меньше боли при конвертации.
-------
Вообще задача конвертации возникает достаточно часто, потому что фреймворки для исследования и продакшена выполняют разные функции: при разработке важнее быстрее итерироваться, пробовать и разрабатывать новое; в продакшене важнее стабильность и производительность. Самый яркий пример --- это внедрение исследовательской модели из PyTorch в среду, где развёрнут TensorRT.
--------
Даже несмотря на то, что в нейронных сетях используется примерно одна и та же математика, её реализация может быть различна. К тому же самые последние наработки могут быть реализованы только в одном из фреймворков. А даже реализованные слои могут иметь различный формат параметров: например в TensorFlow слой для детекций NonMaximumSuppression имеет классическую (hard) реализацию, soft suppression, и прочие.
Это всё исходит из того, что разные фреймворки порождают разные экосистемы, которые развиваются каждая по-своему, в угоду своих задач и особенностей. В PyTorch, например, больше моделей из исследовательких статей, чаще можно встретить какой-то новый слой. А в TensorFlow больше поддержки индустриальных нужд: железа, отладки, производительность.
Из-за этого, конвертаторы часто поддерживают не полную функциональность, и могут требовать для работы более старые версии фреймворков, где они оттестированы.
Также хочется отметить, что производительность модели может сильно варьироваться не только от платформы запуска, но и используемого фреймворка. Поэтому заранее может быть сложно оценить её работу только по информации об используемой модели без предварительных тестов на проде. 
Сериализация моделей в некоторых фреймворках может быть некорректной. Например для сохранения полной информации о модели в PyTorch надо кроме весов pth сохранять также описание графа, желательно вынесенное в отдельный python-модуль, с описанием зависимостей. Иначе если кто-то в forward методе cети поменяет последовательность выполнения, модель может успешно загрузиться, отработать, но результат будет совершенно не тот, что ожидался.
---------
Хорошо, раз задача такая важная и частая, уже обязательно должны быть решения! Действительно, у всех популярных фреймворков есть свои туториалы по экспорту и импорту моделей. Однако в них обычно описывается один не самый сложный случай: модель классификации ImageNet, или (в случае PyTorch) сегментационная модель, которая от классификации отличается наличием Upsampling'а. При этом слоёв, специфичных для детекции, я обычно не могу найти в таких туториалах, уже не говоря про более редкие модели.
И всё равно, в этих конвертациях будет скорее всего зафиксирована не самая актуальные версии фреймворков.

Чтобы не забивать рассказ страшилками и плачем насколько это всё сложно, здесь мы перейдём к тому, что на мой взгляд сильно помогает.
--------
Проект ONNX (Open Neural Network Exchange) создавался командами из Facebook и Microsoft как раз для конвертации между своими фреймворками. Т.е. это изначально фрейморк исключительно для сериализации моделей. Позже к этому подключились другие команды, и сейчас, насколько я знаю, это самый большой проект для описания моделей нейронных сетей.
Кратко про него: сети описываются в виде статического графа с помощью protobuf- сообщений, где хранится название оператора, его связи с другими, а также глобальная мета-информация, включая версию использованных операторов.
Для всего есть актуальная и подробная schema на гитхабе проекта.
---------
Почему ONNX это хорошо, и чем он может помочь? Во-первых, сам проект предоставляет конвертаторы в некоторые фреймворки, кроме непосредственно описания формата. Во-вторых из-за большого коммьюнити редкие или новые фреймворки скорее будут иметь способ импортировать модели ONNX, чем других фреймворков. Фреймворк к тому же активно развивается, пытаясь покрыть все используемые операторы в разных фреймворках. А версионирование операторов гарантирует правильный запуск сериализованной модели в окружении, которое эту версию поддерживает.
--------
При всём при этом ONNX не является панацеей, и иногда лучше обойтись без него. Конечно, если вы живёте внутри одной экосистемы с хорошей сериализацией, вам нет необходимости задумываться про конвертацию. Также бывает, что есть более удобный способ сконвертировать другим конвертатором, не используя формат ONNX. Я не знаю много таких примеров, но самый яркий --- это конвертация из TensorFlow в TFLite. Это разные фреймворки, но TFLite изначально создавался для запуска моделей tensorflow на мобилках, поэтому у него есть хороший конвертатор для этого. Ещё редкий случай --- это когда ваша модель каким-то образом соптимизированна под конкретный фреймворк. Например, вы используете сложный оператор, который можно выразить через несколько простых. Поскольку ONNX ничего не знает про то, из какого фреймворка вы конвертируете, и в какой собираетесь конвертировать потом, он не закладывает никаких платформо- или фреймворко-специфичных оптимизаций. Поэтому вся оптимизация скорее всего пропадёт, и для восстановления нужно будет писать обход уже графа ONNX.
----------
Немного про то, что делать в ONNX очень сложно: это отлаживать, проверять, и редактировать модели напрямую средствами фреймворка. Это делать очень неудобно. В ONNX есть только чекер, который по графу может сказать ОК (модель хорошая), или выдать малопонятную ошибку. Там будет информация про ноду, где всё упало, и даже будет про причину. Но никакой дополнительной информации: почему здесь ожиался такого размера тензор, какие другие входы у оператора, и так далее --- ничего из этого не будет. В итоге может быть быстрее и проще просто пытаться угадывать, что надо в исходной моделе поменять, чтобы ошибка ушла.
----------
Отлично, я только что показал фреймворк для решения задачи унификации фреймворков. Старая проблема, очень много схожего с такими же из других областей. В защиту ONNX могу только сказать, что фреймворк изначально задумывался как универсальный переходник между любыми фреймворками, и эту функцию он пока выполняет лучше других.
----------
Еcли не ONNX, то что? Конечно, есть TensorFlow. Тоже статический граф с версионированием операторов, те же protobuf-сообщения. Очень большая экосистема, с большой поддержкой разного железа. Но у него нет задачи уметь импортировать любые модели. Это как Apple со своими разьёмами Lightning и Thunderbolt. Та же конвертация из ONNX в TensorFlow поддерживается группой из IBM, а не Google.
Есть другой, более старый проект --- MMdnn. Он тоже создавался для перегона нейросетей. Однако его делает малое число людей, и из-за этого поддержка актуальных моделей страдает. К тому же его внутренний формат не стандартизован, так что хранить сериализованные в нём модели может быть опасно.
===========
Покажу пару примеров перевода, с использованием ONNX. Оба они будут про детекторы, правда, но это как раз связано с меньшей поддержкой слоёв, специфичных для детекции.
Начнём с примера в PyTorch. У нас есть детектор ATSS, очень хороший по качеству, oral cvpr этого года, показательный пример модели после исследования. Попробуем его сконвертировать в ONNX и запустить в среде onnxruntime. На самом деле в этом детекторе много сишного кода, который, к счастью, не обязателен, и в моём случае он не использовался. Поэтому можно его выкинуть из репозитория, и, написав небольшую обёртку nn.Module, получим модель, которая корректно работает в PyTorch.
-------
После того, как мы поспользуемся встроенным конвертатором в ONNX, на экран вылезет очень, *очень* много текста. Там будут много странных предупреждений, описание конвертированной модели, описание графа ONNX, но никаких ошибок не выдасться, и конвертатор радостно отрапортует, что всё закончилось успешно.
-------
При этом при попытке запустить модель в onnxruntime вылетит ошибка, что вообще-то получился невалидный граф. И можно видеть, что сообщение об ошибке малоинформативное. Есть название оператора, есть название ошибки. Но непонятно, почему оператор Clip вообще получил тензор типа int64 --- у нас же картинки точно во float32. Непонятно, какой при этом тип ожидается, и какого типа другие входы у оператора. Чтобы разобраться, по-хорошему надо разрезать модель, запускать частями, и выводить необходимую информацию самому, что очень муторно и может вызвать ещё больше путаницы и ошибок. И даже разобравшись, что здесь ожидается тип float32, и надо в PyTorch модель добавить Cast, возникнут ещё ошибки. А после успешного запуска сконвертированной модели, получится совершенно другой от ожидаемого результат.
-------
Почему эти ошибки вообще возникают и как с этим заранее бороться? PyTorch не имеет в себе какого-либо статического описания модели, но его можно построить, фиксируя вызываемые методы во время инференса. Так и делаетсяс помощью trace: отслеживаются вызываемые методы torch.Tensor, и формируются в статический граф. In-place операции при этом будут фиксировать два вызова: сначала выполнение операции, а затем сохранение результата в память. Control flow тоже будет зафиксирован: ветки if'ов будут всегда либо then, либо else, циклы зафиксируют количество итераций. Об этом всём на самом деле и предупреждает trace, когда выводит TraceWarning. Можно исправить исходную модель таким образом, чтобы избавится от возникающих предупреждений и ошибок: добавить явный cast аргументов, использовать индексацию церез бинарную маску, вместо индексов, переписать in-place операциb, и избавиться от конвертации между PyTorch и Numpy. После всех этих операций в итоге получится конвертируемая модель.
Кстати поскольку мы фиксируем запуск на одних данных, проверять работоспособность модели лучше на других, так можно убедиться, что модель при конвертации не создала лишних операций.
---------
Следующий пример несколько старее, но в некоторой степени всё ещё актуален. Будем перегонять Single-Shot Detector из MxNet в TensorFlow. MxNet гораздо ближе по структуре к ONNX и TensorFLow, чем PyTorch: здесь тоже используется статический граф, тоже есть версионирование. И проблем с отслеживанием trace'а у нас не должно возникнуть.
---------
Но зайдя на туториал по экспорту, мы увидим, что MxNet умеет работать только с операторами 7-й версии. Для справки сейчас в стабильной версии ONNX версия операторов 12-я, при этом forward-совместимость иногда ломалась.
-----------
Если посмотреть на конвертатор из ONNX в TensorFlow, можно увидеть, что он может работать с 10-й версией операторов, и нам её будет достаточно, у тому же у всех поддерживаемых операторов в данном случае всё хорошо с forward-совместимостью. Нужно только решить вопрос неподдерживаемыми слоями: генерацию якорей можно перевести в константы, а декодинг предсказаний переписать через общие операции. С Non Maximum Suppression всё сложнее, но...
-----
поскольку это фактически последний слой, его можно обрезать, сконвертировать граф в ONNX, затем в TensorFlow, и уже на конечном шаге добавить в модель. Делать это в TensorFlow всё ещё удобнее, чем в ONNX, особенно учитывая разный формат оператора у ONNX и TensorFlow.
==========
Пора закругляться с докладом. Я хотел сегодня показать, что конвертировать из одного фреймворка в другой приходится часто, и при этом возникающие проблемы могут сильно замедлить внедрение модели. Ещё и сконвертировать может вообще не получиться, поэтому лучше заранее обсуждать, где модель будет разворачиваться, и под какой фреймворк затачиваться. А то будет как с примером из MxNet в TensorFlow.
Также хотелось показать, как с помощью ONNX можно конвертировать относительно без плясок с бубном, если знать некоторые особенности фреймворков. И что иногда можно несколько схитрить, правя граф в нужном фреймворке.
---------
За рамками при этом остались специфичные модели, например Spatial Transformer Network с её Grid Sample; рекурентные модели --- в ONNX нет динамической части, поэтому для сериализации рекурентку придётся разворачивать в несколько повторяющихся сетей; также не рассматривал прочие модели машинного обучения, хотя для моделей sklearn в ONNX есть какая-то поддержка. 
Особые пляски могут возникать вокруг конвертации в платформо-специфичные фреймворки: CoreML, TensorRT, TFLite. Там и поддержка слоёв отстаёт, и формат моделей может быть совсем свой, и прочие интересности.
-------
Спасибо за внимание, буду рад ответить на ваши вопросы!
